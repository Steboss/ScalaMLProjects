{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.num_executors = 3\n",
    "launcher.executor_cores = 3\n",
    "launcher.driver_memory = '2g'\n",
    "launcher.executor_memory = '12g'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to work with a dataset from _Allstate Insurance Company_. The dataset is made of about 300'000 rows, anonymised and masked, with more than 100 categorical and numerical features related to insurances claims. What we have to do it is to understand whether a customer is likely to make a claim or not based on his insurance features history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you are running a notebook in `almond-scala` you have to run this command first to import the right spark libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.3`\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.4.5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing `spark-sql` and `spark-mllib` libraries we can _unzip the input file contained in_ `insurancedata.tar.bz2` . \n",
    "To do that from the Scala interface we are importing `sys.process._` and calling the command line `tar -xjf insurancedata.tar.bz2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at https://datalab-cm-controlnode1.datalab.prod.temp.aws.sb-cloud.net:8090/proxy/application_1584597944032_0001\n",
       "SparkContext available as 'sc' (version = 2.0.0.cloudera2, master = yarn, app id = application_1584597944032_0001)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import sys.process._\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys.process._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n",
       "res0: Int = 0\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"tar -xjf insurancedata.tar.bz2\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can start to import all the libraries needed for this project.  \n",
    "First thing to notice is a substantial difference between Scala and Python or Java as regards the wildcard import. Scala makes a wildcard import with the underscore symbol `_` rather than with asterisk `*`. As a matter of fact `*` can be used as a valid identifier, so that we can write something like `val * = 123`. Thus,to import all the classes from a package we write `import org.apache.spark.sql._`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.Dataset\n",
       "import java.text.SimpleDateFormat\n",
       "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, StringIndexerModel, VectorAssembler}\n",
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.ml.classification.{RandomForestClassifier, RandomForestClassificationModel, LogisticRegression}\n",
       "import org.apache.spark.mllib.evaluation.{BinaryClassificationMetrics, MulticlassMetrics, RegressionMetrics}\n",
       "import org.apache.spark.ml.evaluation.{MulticlassClassificationEvaluator, RegressionEvaluator}\n",
       "import org.apache.spark.ml.linalg.{DenseVector, Vector}\n",
       "import org.apache.spark.sql.{Row, DataFrame}\n",
       "import org.apache.spark.ml.regression.{LinearRegress..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._ //import all the sql framework\n",
    "import org.apache.spark.sql.functions._ //import the sql functions such as expression or dates\n",
    "import org.apache.spark.sql.types._ //import the sql types, like String, Integer and so on \n",
    "\n",
    "\n",
    "import java.text.SimpleDateFormat\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, StringIndexerModel, VectorAssembler}\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.mllib.evaluation.{BinaryClassificationMetrics, MulticlassMetrics, RegressionMetrics}\n",
    "import org.apache.spark.ml.evaluation.{MulticlassClassificationEvaluator, RegressionEvaluator}\n",
    "import org.apache.spark.ml.linalg.{DenseVector, Vector} \n",
    "import org.apache.spark.sql.{Row, DataFrame}\n",
    "import org.apache.spark.ml.regression.{LinearRegression, LinearRegressionModel}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To disable the Spark logging we add the following import statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.log4j.{Level, Logger}\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**if you're running on `almond-scala` access to the spark notebook Session:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    "}.getOrCreate()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have everything set upand we are ready to start with a deep immersion into the Scala functionalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are going to look at each stage of our pre and processing, then we'll write an object to implement all together \n",
    "\n",
    "### Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to deal with the very first spark command. \n",
    "Using the current `spark` session we are going to read the train and test csv file. \n",
    "`option` is a huge attribute, where we can specify how to open the file:\n",
    "- \"header\": if \"true\" we are importing the file's header \n",
    "- \"inferSchema\": if \"true\" spark will try to infer/guess the underlying schema of our file \n",
    "- \"sep\": this specifies the separtor character in our file, it may be a comma \",\" or \";\" or whatever (e.g. (option(\"sep\",\":\")) \n",
    "- \"orc.bloom.filter.columns\": as a further example we can deal with the orc.columns filtering them depending on our choice\n",
    "\n",
    "`format` is in charge of communicating the file format to spark:\n",
    "- \"com.databricks.spark.csv\": csv file\n",
    "- \"json\"\n",
    "- \"parquet\"\n",
    "- \"orc\" \n",
    "\n",
    "then, self-explanatory `load` which takes as an argument the name of the file. \n",
    "Few things should be said about `cache` but I'll write something apart about this command and the methods to cache and persist our data in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainSample: Double = 1.0\n",
       "testSample: Double = 1.0\n",
       "train: String = insurance/train.csv\n",
       "test: String = insurance/test.csv\n",
       "trainInput: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, cat1: string ... 130 more fields]\n",
       "testInput: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, cat1: string ... 129 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//prepocessing data \n",
    "var trainSample = 1.0 \n",
    "var testSample = 1.0\n",
    "val train=\"insurance/train.csv\"\n",
    "val test =\"insurance/test.csv\"\n",
    "\n",
    "val trainInput = spark.read\n",
    "    .option(\"header\",\"true\")\n",
    "    .option(\"inferSchema\",\"true\")\n",
    "    .format(\"com.databricks.spark.csv\")\n",
    "    .load(train)\n",
    "    .cache\n",
    "\n",
    "val testInput  = spark.read\n",
    "    .option(\"header\",\"true\")\n",
    "    .option(\"inferSchema\",\"true\")\n",
    "    .format(\"com.databricks.spark.csv\")\n",
    "    .load(test)\n",
    "    .cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to do a bit of preprocess and feat transformation onto `trainInput`. \n",
    "`withColumnRenamed` is a spark attribute to rename a column into another way and `sample` just select a percentage of the current dataset. In this case `trainSample` and `testSample` are 1.0, which means take the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare the data for training...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, cat1: string ... 130 more fields]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"prepare the data for training...\")\n",
    "var data = trainInput.withColumnRenamed(\"loss\",\"label\").sample(false,trainSample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a minuscle data check for `nan` values. There are tons of way to deal with `nan` but this will be describe later on in the series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null values exist in the dataframe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DF: org.apache.spark.sql.DataFrame = [id: int, cat1: string ... 130 more fields]\n",
       "seed: Long = 12345\n",
       "splits: Array[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = Array([id: int, cat1: string ... 130 more fields], [id: int, cat1: string ... 130 more fields])\n",
       "trainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, cat1: string ... 130 more fields]\n",
       "validationData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, cat1: string ... 130 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var DF = data.na.drop()\n",
    "if (data==DF)\n",
    "    println(\"no null values found in the dataframe\")\n",
    "else {\n",
    "    println(\"null values exist in the dataframe\")\n",
    "    data = DF\n",
    "}\n",
    "val seed = 12345L\n",
    "\n",
    "//here we are splitting the training into a train and validation dataset, 0.75 and 0.25\n",
    "val splits = data.randomSplit(Array(0.75,0.25),seed)\n",
    "val (trainingData, validationData) = (splits(0),splits(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, cat1: string ... 129 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Take a sample for test\n",
    "val testData = testInput.sample(false, testSample).cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature transformation\n",
    "\n",
    "In the following we are implementing few feature transformation functions. I wrongly said function due to my Pyhton background, in Scala something like: \n",
    "``` \n",
    "def ThisIsaMethod(x: Type, y: Type) (var_return: Type): Type = someoperations\n",
    "``` \n",
    "is a Method. On the contrary a function can be something like:\n",
    "```\n",
    "val add = (x: Int, y: Int) => x + y\n",
    "```\n",
    "The following methods are:\n",
    "- `isCateg`: checks if the current column is a categorical one. The categorical columns start with \"cat\" \n",
    "- `categNewCol`: this method take a column name as an input. If `isCateg` return \"true\" then we are creating a string `s\"idx_${c}\"` where `s` is to start a string, \"idx_${c}\" is just a string with the cat-column name \n",
    "- `removeTooManyCategs`: if the columns is \"cat109\" or \"cat110\" or \"cat112\" or \"cat113\" or \"cat116\" and so on we'll remove it from the dataframe \n",
    "- `onlyFeatureCols`: if the columnn name is different from \"id\" or \"label\" return 1. These are the features we are working on, while \"id\" is just an identification number for a particular claim and \"label\" is the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isCateg: (c: String)Boolean\n",
       "categNewCol: (c: String)String\n",
       "removeTooManyCategs: (c: String)Boolean\n",
       "onlyFeatureCols: (c: String)Boolean\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//identify the categorical data\n",
    "def isCateg(c:String): Boolean= c.startsWith(\"cat\") //simply our columns start with cat if they're categorical \n",
    "\n",
    "def categNewCol(c:String): String = if (isCateg(c)) s\"idx_${c}\" else c //if the colum is categorical call it \"idx_COLNUMB\" else COLNUMB\n",
    "def removeTooManyCategs(c: String): Boolean= !(c matches \"cat(109$|110$|112$|113$|116$)\") //remove these columns \n",
    "//nb in the removeTooManyCategs Boolean=! raise an error, be sure that\n",
    "//Boolean= since it's the type we are returning from the function\n",
    "def onlyFeatureCols(c:String): Boolean= !(c matches \"id|label\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are creating then a pipeline in order to get the features we want to work with. Starting from the `trainingData` columns we are removing the unwanted features with `removeTooManyCategs`, collecting only the features that we want to preserver with `onlyFeatureCols` and we are performing a map to `categNewCol`.\n",
    "Making up a pipeline is pivotal in scala programming, as it's a readable and chain way to act which makes our code elegant and reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "featureCols: Array[String] = Array(idx_cat1, idx_cat2, idx_cat3, idx_cat4, idx_cat5, idx_cat6, idx_cat7, idx_cat8, idx_cat9, idx_cat10, idx_cat11, idx_cat12, idx_cat13, idx_cat14, idx_cat15, idx_cat16, idx_cat17, idx_cat18, idx_cat19, idx_cat20, idx_cat21, idx_cat22, idx_cat23, idx_cat24, idx_cat25, idx_cat26, idx_cat27, idx_cat28, idx_cat29, idx_cat30, idx_cat31, idx_cat32, idx_cat33, idx_cat34, idx_cat35, idx_cat36, idx_cat37, idx_cat38, idx_cat39, idx_cat40, idx_cat41, idx_cat42, idx_cat43, idx_cat44, idx_cat45, idx_cat46, idx_cat47, idx_cat48, idx_cat49, idx_cat50, idx_cat51, idx_cat52, idx_cat53, idx_cat54, idx_cat55, idx_cat56, idx_cat57, idx_cat58, idx_cat59, idx_cat60, idx_cat61, idx_cat62, idx_cat63, idx_cat64, idx_cat65, idx_cat66, idx_cat67, idx_cat68, idx_cat69, idx_cat70, i..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Now create a pipeline to clean the columns \n",
    "//these are all teh categorical columns\n",
    "val featureCols = trainingData.columns\n",
    "    .filter(removeTooManyCategs)\n",
    "    .filter(onlyFeatureCols)\n",
    "    .map(categNewCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stringnIndexerStages: Array[org.apache.spark.ml.feature.StringIndexerModel] = Array(strIdx_42a976be3c22, strIdx_aea933f3db11, strIdx_587e76c2dbaf, strIdx_316d27ebd7a0, strIdx_807c37d4885c, strIdx_f3090f7b14f6, strIdx_b6f2e720a374, strIdx_8df18b8bcbb8, strIdx_6b4b1f4c8715, strIdx_cc276b450f8c, strIdx_08b578a611b3, strIdx_42905204bcc9, strIdx_d14caf79b462, strIdx_93891d37be2b, strIdx_c3266a2193d7, strIdx_a474099c197d, strIdx_04e4df3d517d, strIdx_522d83aff814, strIdx_27ba270ee777, strIdx_31e8c46e37fb, strIdx_927b74f1dbbe, strIdx_17260da703aa, strIdx_db67b258b61d, strIdx_63ae1beccb23, strIdx_86f1861c3a79, strIdx_e759731ad184, strIdx_e8967f21624e, strIdx_401c9bf64e8d, strIdx_175f0958b40e, strIdx_0f279d7e0d29, strIdx_6b56e8bb7326, strIdx_3b581007da00, strIdx_dd8b725ce550, strIdx_d17edec38348,..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//now pipeline, take the columns and filter if is categorical, \n",
    "// map (c=> new StringIndexes())\n",
    "val stringnIndexerStages = trainingData.columns\n",
    "    .filter(isCateg)\n",
    "    .map(c=> new StringIndexer()\n",
    "    .setInputCol(c)\n",
    "    .setOutputCol(categNewCol(c))\n",
    "    .fit(trainInput.select(c).union(testInput.select(c))) \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_b988e7d15d02\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//now transform the feature columns into a single vector columne\n",
    "val assembler = new VectorAssembler()\n",
    "    .setInputCols(featureCols)\n",
    "    .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: Linear Regression\n",
    "\n",
    "At this stage things are getting more serious. We are trying to make up an object for the preprocessing which encloses all the previous functionalities we have tested. Then, we are creating a Linear Regression model and chain it with preprocessing in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object Preprocessing\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//define a preprocessing object, which encapsulate all the previous \n",
    "//commands \n",
    "object Preprocessing {\n",
    "  var trainSample = 1.0\n",
    "  var testSample = 1.0\n",
    "  val train = \"insurance/train.csv\"\n",
    "  val test = \"insurance/test.csv\"\n",
    "\n",
    "  println(\"Reading data from \" + train + \" file\")\n",
    "\n",
    "  val trainInput = spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .format(\"com.databricks.spark.csv\")\n",
    "    .load(train)\n",
    "    .cache\n",
    "\n",
    "  val testInput = spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .format(\"com.databricks.spark.csv\")\n",
    "    .load(test)\n",
    "    .cache\n",
    "\n",
    "  println(\"Preparing data for training model\")\n",
    "  var data = trainInput.withColumnRenamed(\"loss\", \"label\").sample(false, trainSample)\n",
    "  var DF = data.na.drop()\n",
    "\n",
    "  // Null check\n",
    "  if (data == DF)\n",
    "    println(\"No null values in the DataFrame\")\n",
    "\n",
    "  else {\n",
    "    println(\"Null values exist in the DataFrame\")\n",
    "    data = DF\n",
    "  }\n",
    "  \n",
    "  val seed = 12345L\n",
    "  val splits = data.randomSplit(Array(0.75, 0.25), seed)\n",
    "  val (trainingData, validationData) = (splits(0), splits(1))\n",
    "\n",
    "  trainingData.cache\n",
    "  validationData.cache\n",
    "\n",
    "  val testData = testInput.sample(false, testSample).cache\n",
    "\n",
    "  def isCateg(c: String): Boolean = c.startsWith(\"cat\")\n",
    "  def categNewCol(c: String): String = if (isCateg(c)) s\"idx_${c}\" else c\n",
    "  // Function to remove categorical columns with too many categories\n",
    "  def removeTooManyCategs(c: String): Boolean = !(c matches \"cat(109$|110$|112$|113$|116$)\")\n",
    "  // Function to select only feature columns (omit id and label)\n",
    "  def onlyFeatureCols(c: String): Boolean = !(c matches \"id|label\")\n",
    "\n",
    "  // Definitive set of feature columns\n",
    "  val featureCols = trainingData.columns\n",
    "    .filter(removeTooManyCategs)\n",
    "    .filter(onlyFeatureCols)\n",
    "    .map(categNewCol)\n",
    "\n",
    "  // StringIndexer for categorical columns (OneHotEncoder should be evaluated as well)\n",
    "  val stringIndexerStages = trainingData.columns.filter(isCateg)\n",
    "      .map(c => new StringIndexer()\n",
    "      .setInputCol(c)\n",
    "      .setOutputCol(categNewCol(c))\n",
    "      .fit(trainInput.select(c).union(testInput.select(c))))\n",
    "\n",
    "  // VectorAssembler for training features\n",
    "  val assembler = new VectorAssembler()\n",
    "    .setInputCols(featureCols)\n",
    "    .setOutputCol(\"features\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the Linear Regression model we want to set its hyper parameters. In particular, we are defining a sequence of values for each of the following parameters, as we want to run a `GridSearch` over the parameter space, in order to find the best linear regression model and results:\n",
    "\n",
    "- `numFolds` : we are performing a k-fold validation, in this case I reduced it to 3 but you are free to increase this number (e.g. 5, 8, 10)\n",
    "- `MaxIter` : this defines the maximum number of iterations the Linear Regression model can run to improve results. If the number of iterations of Linear Regression is greater than `MaxIter` the algorithm will be stopped. Just for sake of computational cost I set `MaxIter` to 100. \n",
    "- `RegParam` : this is the regulatory parameter, to constraint the phase space explored by Linear Regression. It can fluctuate from 1e-8 up to 1e8. As a rule it's better always to test this number in advance and tune it. \n",
    "- `Tol` : this is the tolerance, namely the maximum error we can accept from our results\n",
    "- `ElasticNetParam`: this parameter tune the strenght of the elastic-net penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numFolds: Int = 3\n",
       "MaxIter: Seq[Int] = List(10)\n",
       "RegParam: Seq[Double] = List(0.001)\n",
       "Tol: Seq[Double] = List(1.0E-4)\n",
       "ElasticNetParam: Seq[Double] = List(0.001)\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//define the Linear regression parameters \n",
    "val numFolds= 3 //exploit the k-fold method \n",
    "val MaxIter: Seq[Int] = Seq(100)\n",
    "val RegParam: Seq[Double] = Seq(0.001)\n",
    "val Tol: Seq[Double] = Seq(1e-4)\n",
    "val ElasticNetParam:  Seq[Double] = Seq(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.regression.LinearRegression = linReg_6e918775bd2f\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//define the model estimator\n",
    "val model = new LinearRegression()\n",
    "        .setFeaturesCol(\"features\")\n",
    "        .setLabelCol(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from insurance/train.csv file\n",
      "Preparing data for training model\n",
      "Null values exist in the DataFrame\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_37b81fa2304f\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//build a pipeline estimator \n",
    "val pipeline= new Pipeline()\n",
    "        .setStages(( Preprocessing.stringIndexerStages:+ Preprocessing.assembler) :+model ) \n",
    "// :+ means append"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we can set up the parameter Grid search. `ParamGridBuilder()` is the module to create a grid where we are exploring all the sequential values defined above. `.build()` will create the final grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlinReg_6e918775bd2f-elasticNetParam: 0.001,\n",
       "\tlinReg_6e918775bd2f-maxIter: 10,\n",
       "\tlinReg_6e918775bd2f-regParam: 0.001,\n",
       "\tlinReg_6e918775bd2f-tol: 1.0E-4\n",
       "})\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val paramGrid = new ParamGridBuilder()\n",
    "        .addGrid(model.maxIter, MaxIter)\n",
    "        .addGrid(model.regParam, RegParam)\n",
    "        .addGrid(model.tol,Tol)\n",
    "        .addGrid(model.elasticNetParam, ElasticNetParam)\n",
    "        .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cv: org.apache.spark.ml.tuning.CrossValidator = cv_f2d3c3d16071\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//k-fold to validate the grid params \n",
    "val cv = new CrossValidator()\n",
    "        .setEstimator(pipeline)\n",
    "        .setEvaluator(new RegressionEvaluator)\n",
    "        .setEstimatorParamMaps(paramGrid)\n",
    "        .setNumFolds(numFolds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the grid is set and the k-fold validation - `CrossValidator()`- is set up as well, defining a process pipeling and the `RegressionEvaluator` as model we want to use we can run the Linear Regression model against the `trainingData` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cvModel: org.apache.spark.ml.tuning.CrossValidatorModel = cv_f2d3c3d16071\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//now create the linear regression model \n",
    "val cvModel = cv.fit(Preprocessing.trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics and  best model\n",
    "\n",
    "As a final part we are going to measure the performance of the Logistic Regression against the train dataset in terms of Mean Squared Error and Correlation Coefficient $R^2$. Then, we will be measuring it against the test dataset. \n",
    "As you will see results are not great, given the small training parameters we have set up. As a task you could try to improve this model by modifying the hyperpamaters cell:\n",
    "```\n",
    "//define the Linear regression parameters \n",
    "val numFolds= 3 //exploit the k-fold method \n",
    "val MaxIter: Seq[Int] = Seq(100)\n",
    "val RegParam: Seq[Double] = Seq(0.001)\n",
    "val Tol: Seq[Double] = Seq(1e-4)\n",
    "val ElasticNetParam:  Seq[Double] = Seq(0.001)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on train set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trainPredictionsAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[2873] at rdd at <console>:95\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//find the best model out of the 5 fold \n",
    "println(\"Evaluating model on train set\")\n",
    "val trainPredictionsAndLabels =  \n",
    "    cvModel.transform(Preprocessing.trainingData)\n",
    "    .select(\"label\",\"prediction\")\n",
    "    .map{case Row(label: Double, prediction:Double)\n",
    "    => (label,prediction) }.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainRegressionMetrics: org.apache.spark.mllib.evaluation.RegressionMetrics = org.apache.spark.mllib.evaluation.RegressionMetrics@6deda586\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainRegressionMetrics = new RegressionMetrics(trainPredictionsAndLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.ClassCastException",
     "evalue": " org.apache.spark.ml.tuning.CrossValidatorModel cannot be cast to org.apache.spark.ml.PipelineModel",
     "output_type": "error",
     "traceback": [
      "java.lang.ClassCastException: org.apache.spark.ml.tuning.CrossValidatorModel cannot be cast to org.apache.spark.ml.PipelineModel",
      "  ... 44 elided",
      ""
     ]
    }
   ],
   "source": [
    "val bestModel = cvModel.asInstanceOf[PipelineModel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results: String =\n",
       "\"\n",
       "=================================================================n\n",
       "Param trainSample:1.0\n",
       "\n",
       "Param testSample:1.0\n",
       "\n",
       "=================================================================n\n",
       "Training data MSE =4541073.635702167\n",
       "\n",
       "Training data RMSE=2130.9795014739507\n",
       "\n",
       "Training data R-squared=-0.1674795578921029\n",
       "\"\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//print  the results \n",
    "val results = \n",
    "\"\\n=================================================================n\" +\n",
    "s\"\\nParam trainSample:${Preprocessing.trainSample}\\n\"+ \n",
    "s\"\\nParam testSample:${Preprocessing.testSample}\\n\"+\n",
    "\"\\n=================================================================n\" +\n",
    "s\"\\nTraining data MSE =${trainRegressionMetrics.meanSquaredError}\\n\"+\n",
    "s\"\\nTraining data RMSE=${trainRegressionMetrics.rootMeanSquaredError}\\n\"+\n",
    "s\"\\nTraining data R-squared=${trainRegressionMetrics.r2}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Although results on the train are bad, go on with the test\n"
     ]
    }
   ],
   "source": [
    "println(\"Although results on the train are bad, go on with the test\")\n",
    "cvModel.transform(Preprocessing.testData)\n",
    "    .select(\"id\",\"prediction\")\n",
    "    .withColumnRenamed(\"prediction\",\"loss\")\n",
    "    .coalesce(1) //get all the prediction in a single csv file \n",
    "    .write.format(\"com.databricks.spark.csv\")\n",
    "    .option(\"header\",\"true\")\n",
    "    .save(\"results_LR.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,loss\n",
      "4,1149.2668994674\n",
      "6,2281.771672371282\n",
      "9,11473.14281933248\n",
      "12,4675.344731683625\n",
      "15,-47.40916887730896\n",
      "17,2312.3760003813927\n",
      "21,2622.871263023185\n",
      "28,248.8846099468567\n",
      "32,2562.5745424487345\n"
     ]
    }
   ],
   "source": [
    "!head results_LR.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
